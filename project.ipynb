{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Task 1\n",
    "Fetch MQTTset from Kaggle\n",
    "'''\n",
    "import os\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# Authenticate using Kaggle API\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# Download the dataset\n",
    "dataset = 'cnrieiit/mqttset'\n",
    "api.dataset_download_files(dataset, path='.', unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/10/12 16:41:51 WARN Utils: Your hostname, Jonathan-Laptop resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/10/12 16:41:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/10/12 16:41:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Task 1\n",
    "Setup SparkSession and test connection to Postgres\n",
    "'''\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit \n",
    "\n",
    "\n",
    "# Define dataset, spark constants\n",
    "APPNAME = 'Roberts_Systool_Project'\n",
    "MASTER = 'local[3]'\n",
    "JDBC_JAR_PATH = f\"/home/jrob/.cache/pypoetry/virtualenvs/systems-toolchains-C8y3MhrL-py3.10/lib/python3.10/site-packages/pyspark/jars/postgresql-42.7.4.jar\"\n",
    "JDBC_URL = \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "DB_PROPERTIES = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres_pw\",\n",
    "    \"driver\": \"org.postgresql.Driver\", \n",
    "    \"batchsize\": \"10000\" # trying to optimize the write and seems to have cut the time in half\n",
    "}\n",
    "\n",
    "# Initialize Spark session with PostgreSQL JDBC driver\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(APPNAME) \\\n",
    "    .master(MASTER)\\\n",
    "    .config('spark.driver.host','127.0.0.1')\\\n",
    "    .config(\"spark.jars\", JDBC_JAR_PATH) \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# df = spark.read.jdbc(url=JDBC_URL, table=\"mqtt\", properties=DB_PROPERTIES)\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/12 16:42:17 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+-------+-----------------+--------------------------+--------------------+---------------+----------------------+-------------------+----------------+---------------------+-------------------+------------------+---------------------+-------------+------------+-------------+-----------+--------+--------+----------+------------+--------------+--------------+--------+-----------+------------+---------------+--------+------------+----------------+--------------+------------------+----------+--------+\n",
      "| tcp_flags|tcp_time_delta|tcp_len|mqtt_conack_flags|mqtt_conack_flags_reserved|mqtt_conack_flags_sp|mqtt_conack_val|mqtt_conflag_cleansess|mqtt_conflag_passwd|mqtt_conflag_qos|mqtt_conflag_reserved|mqtt_conflag_retain|mqtt_conflag_uname|mqtt_conflag_willflag|mqtt_conflags|mqtt_dupflag|mqtt_hdrflags|mqtt_kalive|mqtt_len|mqtt_msg|mqtt_msgid|mqtt_msgtype|mqtt_proto_len|mqtt_protoname|mqtt_qos|mqtt_retain|mqtt_sub_qos|mqtt_suback_qos|mqtt_ver|mqtt_willmsg|mqtt_willmsg_len|mqtt_willtopic|mqtt_willtopic_len|    target|set_type|\n",
      "+----------+--------------+-------+-----------------+--------------------------+--------------------+---------------+----------------------+-------------------+----------------+---------------------+-------------------+------------------+---------------------+-------------+------------+-------------+-----------+--------+--------+----------+------------+--------------+--------------+--------+-----------+------------+---------------+--------+------------+----------------+--------------+------------------+----------+--------+\n",
      "|0x00000018|       1.41E-4|     13|                0|                       0.0|                 0.0|            0.0|                   0.0|                0.0|             0.0|                  0.0|                0.0|               0.0|                  0.0|            0|         0.0|   0x00000030|        0.0|    11.0|      32|       0.0|         3.0|           0.0|             0|     0.0|        0.0|         0.0|            0.0|     0.0|         0.0|             0.0|           0.0|               0.0|legitimate|   train|\n",
      "|0x00000018|      0.998962|     13|                0|                       0.0|                 0.0|            0.0|                   0.0|                0.0|             0.0|                  0.0|                0.0|               0.0|                  0.0|            0|         0.0|   0x00000030|        0.0|    11.0|      30|       0.0|         3.0|           0.0|             0|     0.0|        0.0|         0.0|            0.0|     0.0|         0.0|             0.0|           0.0|               0.0|legitimate|   train|\n",
      "|0x00000010|        7.0E-6|      0|                0|                       0.0|                 0.0|            0.0|                   0.0|                0.0|             0.0|                  0.0|                0.0|               0.0|                  0.0|            0|         0.0|            0|        0.0|     0.0|       0|       0.0|         0.0|           0.0|             0|     0.0|        0.0|         0.0|            0.0|     0.0|         0.0|             0.0|           0.0|               0.0|legitimate|   train|\n",
      "|0x00000018|       1.41E-4|     13|                0|                       0.0|                 0.0|            0.0|                   0.0|                0.0|             0.0|                  0.0|                0.0|               0.0|                  0.0|            0|         0.0|   0x00000030|        0.0|    11.0|      31|       0.0|         3.0|           0.0|             0|     0.0|        0.0|         0.0|            0.0|     0.0|         0.0|             0.0|           0.0|               0.0|legitimate|   train|\n",
      "|0x00000010|        9.0E-6|      0|                0|                       0.0|                 0.0|            0.0|                   0.0|                0.0|             0.0|                  0.0|                0.0|               0.0|                  0.0|            0|         0.0|            0|        0.0|     0.0|       0|       0.0|         0.0|           0.0|             0|     0.0|        0.0|         0.0|            0.0|     0.0|         0.0|             0.0|           0.0|               0.0|legitimate|   train|\n",
      "+----------+--------------+-------+-----------------+--------------------------+--------------------+---------------+----------------------+-------------------+----------------+---------------------+-------------------+------------------+---------------------+-------------+------------+-------------+-----------+--------+--------+----------+------------+--------------+--------------+--------+-----------+------------+---------------+--------+------------+----------------+--------------+------------------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+-------+-----------------+--------------------------+--------------------+---------------+----------------------+-------------------+----------------+---------------------+-------------------+------------------+---------------------+-------------+------------+-------------+-----------+--------+--------+----------+------------+--------------+--------------+--------+-----------+------------+---------------+--------+------------+----------------+--------------+------------------+----------+--------+\n",
      "| tcp_flags|tcp_time_delta|tcp_len|mqtt_conack_flags|mqtt_conack_flags_reserved|mqtt_conack_flags_sp|mqtt_conack_val|mqtt_conflag_cleansess|mqtt_conflag_passwd|mqtt_conflag_qos|mqtt_conflag_reserved|mqtt_conflag_retain|mqtt_conflag_uname|mqtt_conflag_willflag|mqtt_conflags|mqtt_dupflag|mqtt_hdrflags|mqtt_kalive|mqtt_len|mqtt_msg|mqtt_msgid|mqtt_msgtype|mqtt_proto_len|mqtt_protoname|mqtt_qos|mqtt_retain|mqtt_sub_qos|mqtt_suback_qos|mqtt_ver|mqtt_willmsg|mqtt_willmsg_len|mqtt_willtopic|mqtt_willtopic_len|    target|set_type|\n",
      "+----------+--------------+-------+-----------------+--------------------------+--------------------+---------------+----------------------+-------------------+----------------+---------------------+-------------------+------------------+---------------------+-------------+------------+-------------+-----------+--------+--------+----------+------------+--------------+--------------+--------+-----------+------------+---------------+--------+------------+----------------+--------------+------------------+----------+--------+\n",
      "|0x00000018|      0.998772|     10|                0|                       0.0|                 0.0|            0.0|                   0.0|                0.0|             0.0|                  0.0|                0.0|               0.0|                  0.0|            0|         0.0|   0x00000030|        0.0|     8.0|      30|       0.0|         3.0|           0.0|             0|     0.0|        0.0|         0.0|            0.0|     0.0|         0.0|             0.0|           0.0|               0.0|legitimate|    test|\n",
      "|0x00000018|      1.000038|     13|                0|                       0.0|                 0.0|            0.0|                   0.0|                0.0|             0.0|                  0.0|                0.0|               0.0|                  0.0|            0|         0.0|   0x00000030|        0.0|    11.0|      31|       0.0|         3.0|           0.0|             0|     0.0|        0.0|         0.0|            0.0|     0.0|         0.0|             0.0|           0.0|               0.0|legitimate|    test|\n",
      "|0x00000018|       1.57E-4|     10|                0|                       0.0|                 0.0|            0.0|                   0.0|                0.0|             0.0|                  0.0|                0.0|               0.0|                  0.0|            0|         0.0|   0x00000030|        0.0|     8.0|      32|       0.0|         3.0|           0.0|             0|     0.0|        0.0|         0.0|            0.0|     0.0|         0.0|             0.0|           0.0|               0.0|legitimate|    test|\n",
      "|0x00000018|        9.2E-5|     13|                0|                       0.0|                 0.0|            0.0|                   0.0|                0.0|             0.0|                  0.0|                0.0|               0.0|                  0.0|            0|         0.0|   0x00000030|        0.0|    11.0|      30|       0.0|         3.0|           0.0|             0|     0.0|        0.0|         0.0|            0.0|     0.0|         0.0|             0.0|           0.0|               0.0|legitimate|    test|\n",
      "|0x00000018|        7.5E-5|      2|                0|                       0.0|                 0.0|            0.0|                   0.0|                0.0|             0.0|                  0.0|                0.0|               0.0|                  0.0|            0|         0.0|   0x000000d0|        0.0|     0.0|       0|       0.0|        13.0|           0.0|             0|     0.0|        0.0|         0.0|            0.0|     0.0|         0.0|             0.0|           0.0|               0.0|legitimate|    test|\n",
      "+----------+--------------+-------+-----------------+--------------------------+--------------------+---------------+----------------------+-------------------+----------------+---------------------+-------------------+------------------+---------------------+-------------+------------+-------------+-----------+--------+--------+----------+------------+--------------+--------------+--------+-----------+------------+---------------+--------+------------+----------------+--------------+------------------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Task 1\n",
    "Ingest data with Spark and write to Postgres\n",
    "\n",
    "Creates a dictionary for data files and associated data category,\n",
    "then loops over both values using .items() method\n",
    "\n",
    "In the loop, it dynamically changes the data category and write mode\n",
    "based on where the code executes in the loop. First loop overwrites\n",
    "to create table and schema if not already exists. Second loop appends\n",
    "\n",
    "Trying optimizations for fun, repartitioning with the rule of thumb of\n",
    "2x the number of cores and using 3 of the 4 cores while leaving 1 for\n",
    "other processes increased speed by about 55-60% from 12mins to about\n",
    "5.75mins. Including batchsize of 10,000 also just about cut the time\n",
    "in half from there down to right at 2 minutes and 45 seconds.\n",
    "'''\n",
    "import os\n",
    "\n",
    "\n",
    "COL_NAMES = [\n",
    "    'tcp_flags', 'tcp_time_delta', 'tcp_len', 'mqtt_conack_flags', \n",
    "    'mqtt_conack_flags_reserved', 'mqtt_conack_flags_sp', 'mqtt_conack_val', \n",
    "    'mqtt_conflag_cleansess', 'mqtt_conflag_passwd', 'mqtt_conflag_qos', \n",
    "    'mqtt_conflag_reserved', 'mqtt_conflag_retain', 'mqtt_conflag_uname', \n",
    "    'mqtt_conflag_willflag', 'mqtt_conflags', 'mqtt_dupflag', 'mqtt_hdrflags', \n",
    "    'mqtt_kalive', 'mqtt_len', 'mqtt_msg', 'mqtt_msgid', 'mqtt_msgtype', \n",
    "    'mqtt_proto_len', 'mqtt_protoname', 'mqtt_qos', 'mqtt_retain', \n",
    "    'mqtt_sub_qos', 'mqtt_suback_qos', 'mqtt_ver', 'mqtt_willmsg', \n",
    "    'mqtt_willmsg_len', 'mqtt_willtopic', 'mqtt_willtopic_len', 'target'\n",
    "]\n",
    "DATA_FOLDER = \"./data/FINAL_CSV\" # won't change due to mqttset zip\n",
    "\n",
    "data_files = {\n",
    "    'train70.csv': 'train',\n",
    "    'test30.csv': 'test'\n",
    "}\n",
    "overwrite = True\n",
    "for file, set_type_value in data_files.items():\n",
    "    file_path = os.path.join(DATA_FOLDER, file)\n",
    "    \n",
    "    # Read CSV file\n",
    "    df = spark.read.csv(file_path, header=False, inferSchema=True).toDF(*COL_NAMES)\n",
    "    \n",
    "    # Add 'set_type' column and dataset value of 'train' or 'test' based on place in dictionary loop\n",
    "    df = df.withColumn('set_type', lit(set_type_value))\n",
    "    \n",
    "    df.repartition(6)\n",
    "    # Show first 5 rows for verification\n",
    "    df.show(5)\n",
    "    \n",
    "    # Overwrite on the first loop and append on the rest\n",
    "    write_mode = 'overwrite' if overwrite else 'append'\n",
    "    \n",
    "    # Write data to Postgres\n",
    "    df.write.jdbc(url=JDBC_URL, \n",
    "                  table=\"mqtt\", \n",
    "                  mode=write_mode, \n",
    "                  properties=DB_PROPERTIES,\n",
    "                  )\n",
    "    \n",
    "    # Set overwrite to False after the first write\n",
    "    if overwrite:\n",
    "        overwrite = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Task 2\n",
    "Ingest Data from Postgres into Spark for Data Analysis\n",
    "'''\n",
    "num_partitions = 6 # Adjust based on your environment\n",
    "mqtt_df = spark.read.jdbc(url=JDBC_URL,\n",
    "                          table=\"mqtt\",\n",
    "                          properties=DB_PROPERTIES,\n",
    "                          numPartitions=num_partitions,\n",
    "                          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: 12081189, 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Number of rows\n",
    "num_rows = mqtt_df.count()\n",
    "\n",
    "# Number of columns\n",
    "num_cols = len(mqtt_df.columns)\n",
    "\n",
    "print(f\"Shape: {num_rows}, {num_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tcp_flags: string (nullable = true)\n",
      " |-- tcp_time_delta: double (nullable = true)\n",
      " |-- tcp_len: integer (nullable = true)\n",
      " |-- mqtt_conack_flags: string (nullable = true)\n",
      " |-- mqtt_conack_flags_reserved: double (nullable = true)\n",
      " |-- mqtt_conack_flags_sp: double (nullable = true)\n",
      " |-- mqtt_conack_val: double (nullable = true)\n",
      " |-- mqtt_conflag_cleansess: double (nullable = true)\n",
      " |-- mqtt_conflag_passwd: double (nullable = true)\n",
      " |-- mqtt_conflag_qos: double (nullable = true)\n",
      " |-- mqtt_conflag_reserved: double (nullable = true)\n",
      " |-- mqtt_conflag_retain: double (nullable = true)\n",
      " |-- mqtt_conflag_uname: double (nullable = true)\n",
      " |-- mqtt_conflag_willflag: double (nullable = true)\n",
      " |-- mqtt_conflags: string (nullable = true)\n",
      " |-- mqtt_dupflag: double (nullable = true)\n",
      " |-- mqtt_hdrflags: string (nullable = true)\n",
      " |-- mqtt_kalive: double (nullable = true)\n",
      " |-- mqtt_len: double (nullable = true)\n",
      " |-- mqtt_msg: string (nullable = true)\n",
      " |-- mqtt_msgid: double (nullable = true)\n",
      " |-- mqtt_msgtype: double (nullable = true)\n",
      " |-- mqtt_proto_len: double (nullable = true)\n",
      " |-- mqtt_protoname: string (nullable = true)\n",
      " |-- mqtt_qos: double (nullable = true)\n",
      " |-- mqtt_retain: double (nullable = true)\n",
      " |-- mqtt_sub_qos: double (nullable = true)\n",
      " |-- mqtt_suback_qos: double (nullable = true)\n",
      " |-- mqtt_ver: double (nullable = true)\n",
      " |-- mqtt_willmsg: double (nullable = true)\n",
      " |-- mqtt_willmsg_len: double (nullable = true)\n",
      " |-- mqtt_willtopic: double (nullable = true)\n",
      " |-- mqtt_willtopic_len: double (nullable = true)\n",
      " |-- target: string (nullable = true)\n",
      " |-- set_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mqtt_df.printSchema()\n",
    "\n",
    "# print target col values\n",
    "# mqtt_df.select(\"target\").distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MQTT message length is: 85.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Task 2 \n",
    "1. Find the average length of an MQTT message captured in\n",
    "the training dataset for a given target X?\n",
    "\n",
    "Also trying to use best practices in defining functions for employers\n",
    "Most analytics will be on the test set when possible to limit processing\n",
    "when possible and should be large enough, but data engineering will require the whole dataset\n",
    "'''\n",
    "from pyspark.sql.functions import col, avg, length\n",
    "\n",
    "\n",
    "def get_average_len(df, set='test', target=None):\n",
    "    '''\n",
    "    Function filters the dataset as much as possible using input parameters\n",
    "    then aggregates the average mqtt message length and returns that integer value\n",
    "\n",
    "    :param set: 'train' or 'test' in mqtt_df\n",
    "    :param target: 'slowite', 'bruteforce', 'flood', 'malformed', 'dos', 'legitimate'\n",
    "    \n",
    "    return: int\n",
    "    '''\n",
    "    if set not in ('test', 'train'):\n",
    "        raise ValueError(\"set parameter must be 'train' or 'test'\")\n",
    "    elif target not in ('slowite', 'bruteforce', 'flood', 'malformed', 'dos', 'legitimate'):\n",
    "        raise ValueError(\"target parameter must be one of these: 'slowite', 'bruteforce', 'flood', 'malformed', 'dos', 'legitimate'\")\n",
    "    \n",
    "    '''\n",
    "    I think it would be get to .cache() and unpersist(), but I'm afraid I'll break local memory\n",
    "    so I'll also filter and select to make dataframe as small as possible beforehand\n",
    "    '''\n",
    "    len_df = df.filter((col('set_type') == set) & (col('target') == target)).select('mqtt_msg').cache()\n",
    "    \n",
    "    # get the right column, then len of values in column, then average, all in an aggregate Spark action method, and .agg() requires .alias()\n",
    "    avg_len = len_df.agg(avg(length(col('mqtt_msg'))).alias('average_length')).collect()\n",
    "    len_df.unpersist()\n",
    "    return avg_len[0]['average_length']\n",
    "\n",
    "print(f\"Average MQTT message length is: {get_average_len(mqtt_df, target='dos'):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/12 17:34:25 WARN CacheManager: Asked to cache already cached data.        \n",
      "24/10/12 17:34:25 WARN CacheManager: Asked to cache already cached data.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average TCP message length for target value slowite is: 1.34 \n",
      " and the most popular header value for slowite is: 0\n",
      "Average TCP message length for target value bruteforce is: 1.51 \n",
      " and the most popular header value for bruteforce is: 0x00000010\n",
      "Average TCP message length for target value flood is: 4.82 \n",
      " and the most popular header value for flood is: 0x00000030\n",
      "Average TCP message length for target value malformed is: 1.79 \n",
      " and the most popular header value for malformed is: 0x00000030\n",
      "Average TCP message length for target value dos is: 2.42 \n",
      " and the most popular header value for dos is: 0x00000040\n",
      "Average TCP message length for target value legitimate is: 1.97 \n",
      " and the most popular header value for legitimate is: 0x00000030\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Task 2 \n",
    "2. For each target value, \n",
    "1) what is the average length of TCP messages and \n",
    "2) what is the most popular header flags’ code (mqtt.hdrflags)?\n",
    "\n",
    "Given the task, it seems no other parameters are necessary\n",
    "'''\n",
    "from pyspark.sql.functions import col, avg, length\n",
    "\n",
    "\n",
    "def get_all_avg_tcp_len(df):\n",
    "    '''\n",
    "    Function loops over each unique value in the target column\n",
    "    to add the average tcp message length to a dictionary and\n",
    "    returns a dictionary with the avg tcp message length for\n",
    "    each target value and that target's most popular hdrflag\n",
    "    placed in a dictionary of tuples where [0]=tcp_len and\n",
    "    [1]=popular_header \n",
    "\n",
    "    Example: {'target':(50, 'hdrflag_val')}\n",
    "\n",
    "    return: dict\n",
    "    '''\n",
    "    answer = {}\n",
    "    targets = [row['target'] for row in mqtt_df.select('target').distinct().collect()]\n",
    "    \n",
    "    # filters for tcp messages and pulls only necessary columns\n",
    "    tcp_df = df.filter((col('tcp_len') != 0)).select('target', 'tcp_len', 'mqtt_hdrflags').cache()\n",
    "    \n",
    "    # Loop over every unique value in target column to get avg tcp_len and top mqtt_hdrflag value\n",
    "    for target in targets:\n",
    "        # Filter by target, then get average length\n",
    "        target_df = tcp_df.filter(col('target') == target).cache()\n",
    "                         \n",
    "        tcp_len = target_df.agg(avg(length(col('tcp_len'))).alias('average_length')).collect()\n",
    "        pop_hdr = target_df.groupBy('mqtt_hdrflags').count().orderBy(col('count').desc()).first()\n",
    "        \n",
    "        target_df.unpersist()\n",
    "        answer[target] = (tcp_len[0]['average_length'], pop_hdr['mqtt_hdrflags'])\n",
    "    # get the right column, then len of values in column, then average, all in an aggregate Spark action method, and .agg() requires .alias()\n",
    "    tcp_df.unpersist()\n",
    "    return answer\n",
    "\n",
    "ans = get_all_avg_tcp_len(mqtt_df)\n",
    "keys = ans.keys()\n",
    "for key in keys:\n",
    "    print(f\"Average TCP message length for target value {key} is: {ans[key][0]:.2f} \\n and the most popular header value for {key} is: {ans[key][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 100:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequenXt flags for tcp_time_delta < Y are:\n",
      "\n",
      "0x00000010\n",
      "0x00000018\n",
      "0x00000011\n",
      "0x00000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "'''\n",
    "3. What is the most frequent X TCP flags for traffic with TCP time delta that is smaller than or equal to Y.\n",
    "- [] X represents a positive integer. Handle scenarios where a user may send\n",
    "negative values to your function.\n",
    "- [] Y represents a float value between 0.0 and 5.0.\n",
    "- [] Make sure to handle this scenario as well: if the user requests 5 most frequent TCP flags but there are 3 Flags that share the same count at rank number 5, please include all of them in your output.\n",
    "\n",
    "Believing that most frequent X means top X(int) TCP Flags and ranks were not asked for, funtion returns a list of flags\n",
    "'''\n",
    "\n",
    "def get_most_frequent_flags(df, X: int, Y: float):\n",
    "    '''\n",
    "    Function filters for tcp_flags and tcp_time_delta to minimize data\n",
    "    for processing. Handles errors by raisng ValueErrors and includes rank\n",
    "    ties as they appear.\n",
    "\n",
    "    return: list\n",
    "    '''\n",
    "    if 0 >= X:\n",
    "        raise ValueError(\"X must be a positive integer.\")\n",
    "    if not (5.0 >= Y >= 0.0):\n",
    "        raise ValueError(\"Y must be a float between 0.0 and 5.0.\")\n",
    "    \n",
    "    # Filter by tcp_time_delta less than Y, then count values in tcp_flags and collect rows\n",
    "    flags_df = df.filter(col('tcp_time_delta') <= Y).select('tcp_flags')\n",
    "    desc_flags = flags_df.groupBy('tcp_flags').count().orderBy(col('count').desc()).collect()\n",
    "\n",
    "    top_flags = []\n",
    "    for i, row in enumerate(desc_flags):\n",
    "        # Continue until X int is reached or if the previous row count equals the current row counts to add ties\n",
    "        if X > i or row['count'] == desc_flags[i - 1]['count']:\n",
    "            top_flags.append(row['tcp_flags'])\n",
    "    return top_flags\n",
    "\n",
    "flags = get_most_frequent_flags(mqtt_df, X=4, Y=.003)\n",
    "print('The most frequenXt flags for tcp_time_delta < Y are:')\n",
    "for flag in flags:\n",
    "    print(f'{flag}') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "systems-toolchains-C8y3MhrL-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
