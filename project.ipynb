{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/10/10 20:17:32 WARN Utils: Your hostname, Jonathan-Laptop resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/10/10 20:17:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/10/10 20:17:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os, zipfile, json, pyspark, requests\n",
    "from io import BytesIO\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Define dataset, spark constants, and kaggle creds path\n",
    "APPNAME = 'Roberts_Systool_Project'\n",
    "MASTER = 'local[3]' # specify Spark to use 3 cores since there are no worker nodes\n",
    "JDBC_JAR_PATH = f\"/home/jrobe/.local/lib/python3.10/site-packages/pyspark/pyspark_jars/postgresql-42.7.4.jar\"\n",
    "JDBC_URL = \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "DB_PROPERTIES = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres_pw\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "KAGGLE_JSON_PATH = \"~/.kaggle/kaggle.json\"\n",
    "KAGGLE__DATASET_URL = f'https://www.kaggle.com/api/v1/datasets/download/cnrieiit/mqttset'\n",
    "AUGMENTED_DATA_FILES = ['train70_augmented.csv', 'test30_augmented.csv']\n",
    "\n",
    "# Define the path to the kaggle.json file\n",
    "kaggle_json_path = os.path.expanduser(KAGGLE_JSON_PATH)\n",
    "\n",
    "# Load the Kaggle credentials\n",
    "with open(kaggle_json_path, 'r') as f:\n",
    "    kaggle_creds = json.load(f)\n",
    "\n",
    "# Extract the username and key\n",
    "KAGGLE_USERNAME = kaggle_creds['username']\n",
    "KAGGLE_KEY = kaggle_creds['key']\n",
    "\n",
    "# Create local host Configuration object for spark \n",
    "config = pyspark.SparkConf()\\\n",
    "    .set('spark.driver.host','127.0.0.1')\\\n",
    "    .setAppName(APPNAME)\\\n",
    "    .setMaster(MASTER)\\\n",
    "    \n",
    "# Initialize Spark session with PostgreSQL JDBC driver\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Kaggle_Spark_PostgreSQL\") \\\n",
    "    .config(conf=config) \\\n",
    "    .config(\"spark.jars\", JDBC_JAR_PATH) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Download the dataset as a zip file in memory\n",
    "response = requests.get(KAGGLE__DATASET_URL, auth=(KAGGLE_USERNAME, KAGGLE_KEY))\n",
    "if response.status_code == 200:\n",
    "    print(\"Dataset downloaded successfully!\")\n",
    "    # Unzip the specific files directly to memory using ZipFile and io and set overwrite to True\n",
    "    dataset_zip = BytesIO(response.content)\n",
    "    overwrite = True\n",
    "    with zipfile.ZipFile(dataset_zip, 'r') as z:\n",
    "        for file in AUGMENTED_DATA_FILES:\n",
    "            if file in z.namelist():\n",
    "                # Load file into memory as a Pandas DataFrame which should be far faster than writing to disk then reading\n",
    "                with z.open(file) as f:\n",
    "                    df = pd.read_csv(f)\n",
    "                    spark_df = spark.createDataFrame(df) # convert pandas to spark\n",
    "                    \n",
    "                    # Show the first 5 rows to verify\n",
    "                    spark_df.show(5)\n",
    "                    \n",
    "                    # Overwrite first file to create schema as well, then append the following files\n",
    "                    if overwrite:\n",
    "                        spark_df.write.jdbc(url=JDBC_URL, table=\"your_table_name\", mode=\"overwrite\", properties=DB_PROPERTIES)\n",
    "                        overwrite = False\n",
    "                    else:\n",
    "                        spark_df.write.jdbc(url=JDBC_URL, table=\"your_table_name\", mode=\"append\", properties=DB_PROPERTIES)\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to download dataset: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
